{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-20T13:30:07.624820Z",
     "start_time": "2020-04-20T13:30:07.423015Z"
    }
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import tiles3 as tc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-20T14:12:50.925208Z",
     "start_time": "2020-04-20T14:12:50.896970Z"
    }
   },
   "outputs": [],
   "source": [
    "env = gym.make('MountainCar-v0').env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-20T14:12:54.344958Z",
     "start_time": "2020-04-20T14:12:54.329025Z"
    }
   },
   "outputs": [],
   "source": [
    "class TileCoder: \n",
    "    def __init__(self, num_tiles, num_tilings, hash_size, position_boundaries, velocity_boundaries):\n",
    "        self.num_tiles = num_tiles\n",
    "        self.num_tilings = num_tilings\n",
    "        self.iht = tc.IHT(hash_size)\n",
    "        \n",
    "        self.position_scale = self.num_tiles / (position_boundaries[1] - position_boundaries[0])\n",
    "        self.velocity_scale = self.num_tiles / (velocity_boundaries[1] - velocity_boundaries[0])\n",
    "        \n",
    "    def get_active_tiles(self, state):\n",
    "        position, velocity = state\n",
    "        state_scaled = [position * self.position_scale, velocity * self.velocity_scale]\n",
    "        \n",
    "        active_tiles = tc.tiles(self.iht, self.num_tilings, state_scaled)\n",
    "        \n",
    "        return np.array(active_tiles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SARSA agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-20T14:12:56.146327Z",
     "start_time": "2020-04-20T14:12:56.127514Z"
    }
   },
   "outputs": [],
   "source": [
    "class SarsaAgent():\n",
    "    def __init__(self, agent_init):\n",
    "        \n",
    "        self.next_tiles = None\n",
    "        self.next_action = None\n",
    "        \n",
    "        self.discount_factor = agent_init[\"discount_factor\"]    \n",
    "        self.learning_rate = agent_init[\"learning_rate\"]\n",
    "        self.epsilon = agent_init[\"epsilon\"]\n",
    "        \n",
    "        self.w = np.ones((agent_init[\"num_action\"], agent_init[\"tile_coder\"][\"hash_size\"]))\n",
    "        self.tile_coder = TileCoder(*agent_init[\"tile_coder\"].values())\n",
    "        \n",
    "    def choose_action(self, state):\n",
    "        \n",
    "        position, velocity = state\n",
    "        active_tiles = self.tile_coder.get_active_tiles(state)\n",
    "        \n",
    "        action_values = np.zeros(env.action_space.n)\n",
    "        for action in range(env.action_space.n):\n",
    "            action_values[action] = np.sum(self.w[action][active_tiles])\n",
    "            \n",
    "        if np.random.rand() < self.epsilon:\n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "            action = np.argmax(action_values)\n",
    "            \n",
    "        self.next_action = action\n",
    "        self.next_tiles = active_tiles\n",
    "            \n",
    "        return action\n",
    "        \n",
    "    def update(self, state, reward, done):\n",
    "        if not done:\n",
    "            self.update_step(state, reward)\n",
    "        else:\n",
    "            self.update_end(state, reward)\n",
    "        \n",
    "    def update_step(self, next_state, reward):\n",
    "        \n",
    "        current_action = self.next_action\n",
    "        current_tiles = self.next_tiles\n",
    "        \n",
    "        self.choose_action(next_state)\n",
    "        \n",
    "        target = reward + self.discount_factor * np.sum(self.w[self.next_action, self.next_tiles])\\\n",
    "        - np.sum(self.w[current_action, current_tiles])\n",
    "        self.w[current_action, current_tiles] += self.learning_rate * target\n",
    "        \n",
    "    def update_end(self, state, reward):\n",
    "        current_action = self.next_action\n",
    "        current_tiles = self.next_tiles\n",
    "        \n",
    "        target = reward - np.sum(self.w[current_action][current_tiles])\n",
    "        self.w[current_action][current_tiles] += self.learning_rate * target\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-21T13:06:57.014374Z",
     "start_time": "2020-04-21T13:06:56.915736Z"
    }
   },
   "outputs": [],
   "source": [
    "position_boundaries = (env.observation_space.low[0], env.observation_space.high[0])\n",
    "velocity_boundaries = (env.observation_space.low[1], env.observation_space.high[1])\n",
    "\n",
    "agent_init = {\n",
    "    'discount_factor': 0.9, \n",
    "    'learning_rate': 0.01, \n",
    "    'epsilon': 0.01,\n",
    "    'num_action': env.action_space.n,\n",
    "    'tile_coder': {\n",
    "        'num_tiles': 8,\n",
    "        'num_tilings': 20,\n",
    "        'hash_size': 4096,\n",
    "        'position_boundaries': position_boundaries, \n",
    "        'velocity_boundaries': velocity_boundaries\n",
    "    }\n",
    "}\n",
    "sarsa_agent = SarsaAgent(agent_init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-21T13:07:04.427758Z",
     "start_time": "2020-04-21T13:06:57.082185Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-9192bdf6399f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0msarsa_agent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoose_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_max_per_episode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m         \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msarsa_agent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext_action\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0msarsa_agent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/gym/envs/classic_control/mountain_car.py\u001b[0m in \u001b[0;36mrender\u001b[0;34m(self, mode)\u001b[0m\n\u001b[1;32m    117\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcartrans\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_rotation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcos\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mpos\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mviewer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreturn_rgb_array\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m'rgb_array'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_keys_to_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/gym/envs/classic_control/rendering.py\u001b[0m in \u001b[0;36mrender\u001b[0;34m(self, return_rgb_array)\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_rgb_array\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0mglClearColor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwindow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwindow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mswitch_to\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwindow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_events\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pyglet/window/__init__.py\u001b[0m in \u001b[0;36mclear\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1226\u001b[0m         \u001b[0mbuffer\u001b[0m\u001b[0;34m.\u001b[0m  \u001b[0mThe\u001b[0m \u001b[0mwindow\u001b[0m \u001b[0mmust\u001b[0m \u001b[0mbe\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mactive\u001b[0m \u001b[0mcontext\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msee\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mswitch_to\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1227\u001b[0m         \"\"\"\n\u001b[0;32m-> 1228\u001b[0;31m         \u001b[0mgl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglClear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGL_COLOR_BUFFER_BIT\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0mgl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGL_DEPTH_BUFFER_BIT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1229\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1230\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdispatch_event\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pyglet/gl/lib.py\u001b[0m in \u001b[0;36merrcheck\u001b[0;34m(result, func, arguments)\u001b[0m\n\u001b[1;32m     83\u001b[0m     \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m \u001b[0;32mdef\u001b[0m \u001b[0merrcheck\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marguments\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_debug_gl_trace\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "timesteps_per_episode = list()\n",
    "timesteps_per_episode_per_chunk = list()\n",
    "trunk_size = 20\n",
    "n_episode = 200\n",
    "t_max_per_episode = 200\n",
    "\n",
    "assert n_episode % trunk_size == 0, f\"trunk_size ({trunk_size}) must be a divisor of n_episode ({n_episode})\"\n",
    "\n",
    "for i_episode in range(n_episode):\n",
    "    state = env.reset()\n",
    "    sarsa_agent.choose_action(state)\n",
    "    for t in range(t_max_per_episode):\n",
    "        env.render()\n",
    "        state, reward, done, info = env.step(sarsa_agent.next_action)\n",
    "        sarsa_agent.update(state, reward, done)\n",
    "        if done:\n",
    "            break\n",
    "    if i_episode % trunk_size == 0:\n",
    "        print(i_episode)\n",
    "    timesteps_per_episode.append(t)\n",
    "    \n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-17T16:17:18.071865Z",
     "start_time": "2020-04-17T16:17:18.015108Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([252.6 , 248.05, 272.85, 257.1 , 242.45, 237.55, 233.  , 216.3 ,\n",
       "       244.1 , 217.2 ])"
      ]
     },
     "execution_count": 251,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "timesteps_per_episode = np.array(timesteps_per_episode)\n",
    "timesteps_per_episode.reshape(trunk_size, int(n_episode/trunk_size)).mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-17T15:58:20.385507Z",
     "start_time": "2020-04-17T15:58:20.368384Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(450,)"
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "timesteps_per_episode[0:450].shapec#\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q-Learning Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-20T14:48:54.210132Z",
     "start_time": "2020-04-20T14:48:54.149543Z"
    }
   },
   "outputs": [],
   "source": [
    "class QLearningAgent():\n",
    "    def __init__(self, agent_init):\n",
    "        \n",
    "        self.current_tiles = None\n",
    "        self.current_action = None\n",
    "        \n",
    "        self.discount_factor = agent_init[\"discount_factor\"]    \n",
    "        self.learning_rate = agent_init[\"learning_rate\"]\n",
    "        self.epsilon = agent_init[\"epsilon\"]\n",
    "        \n",
    "        self.w = np.ones((agent_init[\"num_action\"], agent_init[\"tile_coder\"][\"hash_size\"]))\n",
    "        self.tile_coder = TileCoder(*agent_init[\"tile_coder\"].values())\n",
    "        \n",
    "    def choose_action(self, state):\n",
    "        \n",
    "        if self.current_tiles is None:\n",
    "            active_tiles = self.tile_coder.get_active_tiles(state)\n",
    "        else:\n",
    "            active_tiles = self.current_tiles\n",
    "        \n",
    "        action_values = np.zeros(env.action_space.n)\n",
    "        for action in range(env.action_space.n):\n",
    "            action_values[action] = np.sum(self.w[action][active_tiles])\n",
    "            \n",
    "        if np.random.rand() < self.epsilon:\n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "            action = np.argmax(action_values)\n",
    "            \n",
    "        if self.current_tiles is None:\n",
    "            self.current_tiles = np.copy(active_tiles)\n",
    "            \n",
    "        self.current_action = action\n",
    "            \n",
    "        return action\n",
    "        \n",
    "    def update(self, next_state, reward, done):\n",
    "\n",
    "        if not done:\n",
    "            self.update_step(next_state, reward)\n",
    "        else:\n",
    "            self.update_end(next_state, reward)\n",
    "            \n",
    "    def update_step(self, next_state, reward):\n",
    "        \n",
    "        next_active_tiles = self.tile_coder.get_active_tiles(next_state)\n",
    "        \n",
    "        target = reward + self.discount_factor * np.max(self.w[:, next_active_tiles], axis=0).sum() \\\n",
    "        - np.sum(self.w[self.current_action][self.current_tiles])\n",
    "        self.w[self.current_action][self.current_tiles] += self.learning_rate * target\n",
    "        \n",
    "        self.current_tiles = next_active_tiles\n",
    "        \n",
    "    def update_end(self, state, reward):\n",
    "        target = reward - np.sum(self.w[self.current_action][self.current_tiles])\n",
    "        self.w[self.current_action][self.current_tiles] += self.learning_rate * target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-20T14:48:54.813476Z",
     "start_time": "2020-04-20T14:48:54.798876Z"
    }
   },
   "outputs": [],
   "source": [
    "position_boundaries = (env.observation_space.low[0], env.observation_space.high[0])\n",
    "velocity_boundaries = (env.observation_space.low[1], env.observation_space.high[1])\n",
    "\n",
    "agent_init = {\n",
    "    'discount_factor': 0.9, \n",
    "    'learning_rate': 0.01, \n",
    "    'epsilon': 0.01,\n",
    "    'num_action': env.action_space.n,\n",
    "    'tile_coder': {\n",
    "        'num_tiles': 8,\n",
    "        'num_tilings': 20,\n",
    "        'hash_size': 4096,\n",
    "        'position_boundaries': position_boundaries, \n",
    "        'velocity_boundaries': velocity_boundaries\n",
    "    }\n",
    "}\n",
    "q_learning_agent = QLearningAgent(agent_init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-20T15:01:35.175194Z",
     "start_time": "2020-04-20T14:48:55.592429Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "20\n",
      "40\n",
      "60\n",
      "80\n",
      "100\n",
      "120\n",
      "140\n",
      "160\n",
      "180\n"
     ]
    }
   ],
   "source": [
    "timesteps_per_episode = list()\n",
    "timesteps_per_episode_per_chunk = list()\n",
    "trunk_size = 20\n",
    "n_episode = 200\n",
    "t_max_per_episode = 200\n",
    "\n",
    "assert n_episode % trunk_size == 0, f\"trunk_size ({trunk_size}) must be a divisor of n_episode ({n_episode})\"\n",
    "\n",
    "for i_episode in range(n_episode):\n",
    "    state = env.reset()\n",
    "    for t in range(t_max_per_episode):\n",
    "        env.render()\n",
    "        action = q_learning_agent.choose_action(state)\n",
    "        next_state, reward, done, info = env.step(action)\n",
    "        q_learning_agent.update(next_state, reward, done) \n",
    "        if done:\n",
    "            break\n",
    "    if i_episode % trunk_size == 0:\n",
    "        print(i_episode)\n",
    "    timesteps_per_episode.append(t)\n",
    "    \n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-20T15:01:56.804825Z",
     "start_time": "2020-04-20T15:01:56.777265Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([223.6 , 269.1 , 220.35, 273.15, 214.95, 210.85, 210.95, 211.55,\n",
       "       211.2 , 222.9 ])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "timesteps_per_episode = np.array(timesteps_per_episode)\n",
    "timesteps_per_episode.reshape(trunk_size, int(n_episode/trunk_size)).mean(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-20T13:28:26.989232Z",
     "start_time": "2020-04-20T13:28:26.974093Z"
    }
   },
   "source": [
    "### Expected SARSA agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-20T15:45:09.898409Z",
     "start_time": "2020-04-20T15:45:09.873193Z"
    }
   },
   "outputs": [],
   "source": [
    "class ExpectedSarsaAgent():\n",
    "    def __init__(self, agent_init):\n",
    "        \n",
    "        self.current_tiles = None\n",
    "        self.current_action = None\n",
    "        \n",
    "        self.discount_factor = agent_init[\"discount_factor\"]    \n",
    "        self.learning_rate = agent_init[\"learning_rate\"]\n",
    "        self.epsilon = agent_init[\"epsilon\"]\n",
    "        \n",
    "        self.w = np.ones((agent_init[\"num_action\"], agent_init[\"tile_coder\"][\"hash_size\"]))\n",
    "        self.tile_coder = TileCoder(*agent_init[\"tile_coder\"].values())\n",
    "        \n",
    "    def choose_action(self, state):\n",
    "        \n",
    "        if self.current_tiles is None:\n",
    "            active_tiles = self.tile_coder.get_active_tiles(state)\n",
    "        else:\n",
    "            active_tiles = self.current_tiles\n",
    "        \n",
    "        action_values = np.zeros(env.action_space.n)\n",
    "        for action in range(env.action_space.n):\n",
    "            action_values[action] = np.sum(self.w[action][active_tiles])\n",
    "            \n",
    "        if np.random.rand() < self.epsilon:\n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "            action = np.argmax(action_values)\n",
    "            \n",
    "        if self.current_tiles is None:\n",
    "            self.current_tiles = np.copy(active_tiles)\n",
    "            \n",
    "        self.current_action = action\n",
    "            \n",
    "        return action\n",
    "        \n",
    "    def update(self, next_state, reward, done):\n",
    "\n",
    "        if not done:\n",
    "            self.update_step(next_state, reward)\n",
    "        else:\n",
    "            self.update_end(next_state, reward)\n",
    "            \n",
    "    def update_step(self, next_state, reward):\n",
    "        \n",
    "        next_active_tiles = self.tile_coder.get_active_tiles(next_state)\n",
    "        \n",
    "        target = reward + self.discount_factor * np.mean(self.w[:, next_active_tiles].sum(axis=1)) \\\n",
    "        - np.sum(self.w[self.current_action][self.current_tiles])\n",
    "        self.w[self.current_action][self.current_tiles] += self.learning_rate * target\n",
    "        \n",
    "        self.current_tiles = next_active_tiles\n",
    "        \n",
    "    def update_end(self, state, reward):\n",
    "        target = reward - np.sum(self.w[self.current_action][self.current_tiles])\n",
    "        self.w[self.current_action][self.current_tiles] += self.learning_rate * target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-20T16:14:39.674294Z",
     "start_time": "2020-04-20T16:14:39.630503Z"
    }
   },
   "outputs": [],
   "source": [
    "position_boundaries = (env.observation_space.low[0], env.observation_space.high[0])\n",
    "velocity_boundaries = (env.observation_space.low[1], env.observation_space.high[1])\n",
    "\n",
    "agent_init = {\n",
    "    'discount_factor': 0.9, \n",
    "    'learning_rate': 0.01, \n",
    "    'epsilon': 0.01,\n",
    "    'num_action': env.action_space.n,\n",
    "    'tile_coder': {\n",
    "        'num_tiles': 8,\n",
    "        'num_tilings': 20,\n",
    "        'hash_size': 4096,\n",
    "        'position_boundaries': position_boundaries, \n",
    "        'velocity_boundaries': velocity_boundaries\n",
    "    }\n",
    "}\n",
    "expected_sarsa_agent = ExpectedSarsaAgent(agent_init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-20T16:35:33.191820Z",
     "start_time": "2020-04-20T16:14:40.126873Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "20\n",
      "40\n",
      "60\n",
      "80\n",
      "100\n",
      "120\n",
      "140\n",
      "160\n",
      "180\n",
      "200\n",
      "220\n",
      "240\n",
      "260\n",
      "280\n",
      "300\n",
      "320\n",
      "340\n",
      "360\n",
      "380\n"
     ]
    }
   ],
   "source": [
    "timesteps_per_episode = list()\n",
    "timesteps_per_episode_per_chunk = list()\n",
    "trunk_size = 20\n",
    "n_episode = 400\n",
    "t_max_per_episode = 200\n",
    "\n",
    "assert n_episode % trunk_size == 0, f\"trunk_size ({trunk_size}) must be a divisor of n_episode ({n_episode})\"\n",
    "\n",
    "for i_episode in range(n_episode):\n",
    "    state = env.reset()\n",
    "    for t in range(t_max_per_episode):\n",
    "        env.render()\n",
    "        action = expected_sarsa_agent.choose_action(state)\n",
    "        next_state, reward, done, info = env.step(action)\n",
    "        expected_sarsa_agent.update(next_state, reward, done) \n",
    "        if done:\n",
    "            break\n",
    "    if i_episode % trunk_size == 0:\n",
    "        print(i_episode)\n",
    "    timesteps_per_episode.append(t)\n",
    "    \n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-20T15:57:04.499465Z",
     "start_time": "2020-04-20T15:57:04.469525Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([197.7 , 192.6 , 198.15, 198.35, 197.05, 197.  , 196.45, 193.55,\n",
       "       197.6 , 197.7 ])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "timesteps_per_episode = np.array(timesteps_per_episode)\n",
    "timesteps_per_episode.reshape(trunk_size, int(n_episode/trunk_size)).mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-21T13:07:07.557779Z",
     "start_time": "2020-04-21T13:07:07.536197Z"
    }
   },
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
